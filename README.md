# Toward Artificial Palpation: Representation Learning of Touch on Soft Bodies
Official implementation of NeurIPS 2025 paper: "Toward Artificial Palpation: Representation Learning of Touch on Soft Bodies"

Here you can find:
1. PalpationSim - A 2D FEM simulation for soft object tactile interaction. While the simulation is not meant to be realistic, it is a great tool for quick hypothesis testing.
2. Data - We have open sourced all the simulation and Xela data that was used in our paper.
3. Representation Learning - Models and training code for learning a representation for soft-object touch sequences.
4. Downstream Tasks - Mainly, MRI reconstruction from the frozen representation. 

Check out the [Paper](https://arxiv.org/pdf/2511.16596) and our [Website](https://zoharri.github.io/artificial-palpation).

<div float="center">
<img src="assets/palpation_demo.gif"/>
</div>

## Code  Overview

### PalpationSim
In the simulation dir:
- `soft_object_sim.py`: Contains the implementation of the simulator class, which runs a simulation of a general soft object and a probe.
- `breast_palpation.py`: Specifies the vertices of the breast model.
- `shapes.py` : Contains the main implementation of a soft body, and how to calculate its internal elastic energy. There are several methods, either based on spring-mass systems or based on the finite element method. This file also creates the different probe shapes.
- `trajectory.py` : Contains a class that generates trajectories for the probe (different trajectory types are supported).
- `config.yaml` : Contains the parameters for a simulation run
- `configs.py` : Contains the dataclass that defines the simulation config
- `run_experiment.py` : Contains the main script for running the simulation. Takes as input a config yaml file.
- `auto_create_experiments.py` : A script for creating multiple experiments data in a data folder. Each experiment is given a folder name experiment_i, with a procedurally generated config file (based on modifying the base config.yaml). 
- `preprocess_data.py` : Preprocesses the data generated by auto_create_experiments into a dataset for training

### Models
In the models dir:
- downstream dir : All the downstream models and metrics. Including MRI predictions, regression heads and shell classification. 
- representation_learning dir: Representation learning models, including force reconstruction and force map baseline. 

### Datasets
- `simulation_dataset.py` : Provides a PyTorch dataset for images + sensor observations stored in an HDF5 file (for simulation data)
- `real_dataset.py` : Provides a pytorch dataset for images + sensor observations stored in HDF5 files (for real data)

- Other:
- `train_rep_and_downstream.py` : Main training script for the representation and downstream
- `real_data_utils.py` : Utils file for handling the real data
- `train_utils.py` : General utils file
- `requirements.txt` : Dependencies requirements file

## Installation
For training, you will need a GPU, but the simulation runs on CPU only.
To create an environment, run the following commands:
```bash
conda create -n palp python=3.9
conda activate palp
pip install -r requirements.txt
```

## Data
We release both our [Real Dataset](https://zenodo.org/records/17608184) and [Simulation Dataset](https://zenodo.org/records/17652264), which were used in the paper.

The real dataset contains more than 250 hours of soft body tactile interactions!

In the simulation dataset you can also find a small subset of the data, which is useful for debugging. 

## First Steps

### Download Data
Download the data from the above links (either real or simulated). Alternatively, you can run the simulation by yourself by following the instruction above. 

### Run Simulation
IF YOU HAVE DOWNLOADED THE DATA YOU CAN SKIP THIS STEP.
To run the simulation, first move into the simulation dir:

```bash
cd simulation
```
To run a demo of the simulation, which will run two poke trajectories, run the following:
```bash
python run_experiment.py --config_path config.yaml
```

<div float="center">
<img src="assets/palpation_sim.gif"/>
</div>

You can either pass arguments via the command line or in the config.yaml file. To get full details on available arguments, run:
```bash
python run_experiment.py --help
```

To run a multiprocess full data collection (as described in the paper), run:
```bash
python auto_create_experiments.py --data_folder /path/to/data/root
```
Where again, you can use the --help flag to see possible command line arguments for the data collection procedure.

After collecting data, the dataset need to be preprocessed before training. To do so, simply run:
```bash
python preprocess_data.py --root_dir /path/to/data/root --output_file /path/to/data.h5
```

## Run Training
Wandb is required to run training.
Use the --help flag to find out about all command line parameter options. 

### Representation Learning
To run a basic self-supervised learning:

```bash
python train_rep_and_downstream.py --config_path configs/GRU_reconstruction_model_real.yaml --dataset.data_folder path/to/data --optimizer.disable_downstream_model True --models.rep_learning_model.name ReconstructionModel
```

And simulation training (same training script):
```bash
python train_rep_and_downstream.py --config_path configs/GRU_reconstruction_model_sim.yaml --dataset.data_folder /path/to --dataset.h5_name data.h5  --optimizer.disable_downstream_model True --models.rep_learning_model.name ReconstructionModel
```

### Downstream Task
To run a basic flow matching image prediction, based on the pretrained representation model:
```bash
python train_rep_and_downstream.py --config_path configs/GRU_reconstruction_model_real.yaml --dataset.data_folder path/to/data --models.downstream_model.flow_steps 20 --models.downstream_model.name FlowMatchingImagePred --models.downstream_model.num_channels 16 --optimizer.freeze_rep_learning_model true --models.rep_learning_model_checkpoint wandb/ENTER_RUN_ID/files/best_rep_model.pt
```
If you trained the representation model with the above command, the model should be saved in a wandb run. You can find the run id in the wandb website.

## Cite
If you found our work useful, here is how you can cite it:
```
 @inproceedings{rimontoward,
      title={Toward Artificial Palpation: Representation Learning of Touch on Soft Bodies},
      author={Rimon, Zohar and Shafer, Elisei and Tepper, Tal and Shimron, Efrat and Tamar, Aviv},
      booktitle={The Thirty-ninth Annual Conference on Neural Information Processing Systems}
    }  
```

## Contact
If you found this interesting, or if you have any question, feel free to reach out: zohar dot rimon at campus dot technion dot ac dot il.

